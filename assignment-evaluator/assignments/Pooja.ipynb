{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision And Image Analytics"
      ],
      "metadata": {
        "id": "mZKKY2YPvb0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Description Object Detection using YOLOv8 and Faster R-CNN"
      ],
      "metadata": {
        "id": "Bc912IJevsGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "\n",
        "Implement and compare YOLOv8 and Faster R-CNN for wheat head detection in terms of accuracy, speed, and robustness.\n"
      ],
      "metadata": {
        "id": "3h1sM5UcvvDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 â€” Environment setup (Colab)\n",
        "# 1) Check GPU\n",
        "!nvidia-smi || echo \"No GPU found. In Colab, set: Runtime > Change runtime type > GPU\"\n",
        "\n",
        "# 2) Install key libs\n",
        "# - ultralytics (YOLOv8)\n",
        "# - torchmetrics (for mAP on Faster R-CNN)\n",
        "# - pycocotools (COCO metrics support)\n",
        "# - opencv-python (image I/O)\n",
        "!pip -q install ultralytics==8.3.0 torchmetrics==1.4.1 pycocotools opencv-python\n",
        "\n",
        "# 3) Quick sanity imports (should print versions, not error)\n",
        "import torch, torchvision, cv2, ultralytics, torchmetrics\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "print(\"OpenCV:\", cv2.__version__)\n",
        "print(\"ultralytics:\", ultralytics.__version__)\n",
        "print(\"torchmetrics:\", torchmetrics.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnwlgdUu9khy",
        "outputId": "1c7d7937-34f5-4f00-9ad9-6eecf6e032bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 20 07:11:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0             30W /   70W |     242MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "torch: 2.8.0+cu126\n",
            "torchvision: 0.23.0+cu126\n",
            "OpenCV: 4.11.0\n",
            "ultralytics: 8.3.0\n",
            "torchmetrics: 1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file \"/content/global-wheat-detection.zip\"\n",
        "!ls -lh \"/content/global-wheat-detection.zip\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VFuifcIa_CA",
        "outputId": "eeb075a1-dadb-4000-a4b6-dd8bfdf7ac83"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/global-wheat-detection.zip: Zip archive data, at least v4.5 to extract, compression method=deflate\n",
            "-rw-r--r-- 1 root root 19M Sep 20 10:02 /content/global-wheat-detection.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c global-wheat-detection -p /content/\n",
        "!unzip -q /content/global-wheat-detection.zip -d /content/global-wheat-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB4Dj3sJbMTH",
        "outputId": "0dab4e5a-1955-4ef6-87ab-1d2eedf97e35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n",
            "[/content/global-wheat-detection.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/global-wheat-detection.zip or\n",
            "        /content/global-wheat-detection.zip.zip, and cannot find /content/global-wheat-detection.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/train.zip\" -d /content/global-wheat-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8mmJoIBlFMo",
        "outputId": "f2c208e4-326d-4482-d7bb-e5b2361fc64b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/train.zip, /content/train.zip.zip or /content/train.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/train.zip\" -d /content/global-wheat-detection.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3wpAJ9klwcZ",
        "outputId": "c43ca1f1-64de-41bc-b654-6d62dfe07a3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[/content/train.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/train.zip or\n",
            "        /content/train.zip.zip, and cannot find /content/train.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2c â€” Verify & extract archives robustly (tries global-wheat-detection.zip first)\n",
        "\n",
        "import os, subprocess, glob, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content\")\n",
        "TARGET = Path(\"/content/global-wheat-detection\")\n",
        "TARGET.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def sh(cmd):\n",
        "    rc = subprocess.call(cmd, shell=True)\n",
        "    return rc\n",
        "\n",
        "def zip_ok(p: Path) -> bool:\n",
        "    # Test archive integrity\n",
        "    return sh(f'unzip -t \"{p}\" > /dev/null') == 0\n",
        "\n",
        "print(\"ðŸ“‚ Looking for dataset archives in /content ...\")\n",
        "candidates = []\n",
        "for name in [\"global-wheat-detection.zip\", \"train.zip\"]:\n",
        "    p = BASE / name\n",
        "    if p.exists():\n",
        "        candidates.append(p)\n",
        "\n",
        "print(\"Found:\", [c.name for c in candidates])\n",
        "\n",
        "# Try global-wheat-detection.zip first (contains CSV + train/)\n",
        "archive = None\n",
        "for cand in candidates:\n",
        "    print(f\"ðŸ”Ž Testing {cand.name} ...\")\n",
        "    if zip_ok(cand):\n",
        "        archive = cand\n",
        "        print(f\"âœ… Zip integrity OK: {cand.name}\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"âŒ Zip test failed: {cand.name}\")\n",
        "\n",
        "if archive is None:\n",
        "    raise RuntimeError(\n",
        "        \"No valid zip archive found. Please re-upload 'global-wheat-detection.zip' \"\n",
        "        \"(~1.6GB) OR re-upload a good 'train.zip'.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nðŸ“¦ Extracting {archive.name} ... this may take a minute ...\")\n",
        "rc = sh(f'unzip -oq \"{archive}\" -d \"{TARGET}\"')\n",
        "if rc != 0:\n",
        "    raise RuntimeError(f\"Unzip failed with exit code {rc} for {archive.name}\")\n",
        "\n",
        "# Sanity check structure\n",
        "csv_path = TARGET / \"train.csv\"\n",
        "train_dir = TARGET / \"train\"\n",
        "\n",
        "# Some zips may create nested directory; try to locate if not at root\n",
        "if not train_dir.exists():\n",
        "    # Search for a 'train' folder inside\n",
        "    nested = [p for p in TARGET.rglob(\"train\") if p.is_dir()]\n",
        "    if nested:\n",
        "        # Pick the one with most jpgs\n",
        "        best = None; bestn = -1\n",
        "        for d in nested:\n",
        "            n = len(list(d.glob(\"*.jpg\")))\n",
        "            if n > bestn:\n",
        "                best, bestn = d, n\n",
        "        if best and best != train_dir:\n",
        "            train_dir.mkdir(exist_ok=True, parents=True)\n",
        "            print(f\"Normalizing: copying train images from {best} -> {train_dir}\")\n",
        "            shutil.copytree(best, train_dir, dirs_exist_ok=True)\n",
        "\n",
        "# Count images\n",
        "jpgs = list(train_dir.glob(\"*.jpg\")) if train_dir.exists() else []\n",
        "print(f\"\\nðŸ–¼ï¸ Images found in {train_dir}: {len(jpgs)}\")\n",
        "\n",
        "# CSV: ensure present (you may have uploaded it earlier separately)\n",
        "if not csv_path.exists():\n",
        "    # Try to find a train.csv somewhere under TARGET and move it up\n",
        "    candidates_csv = list(TARGET.rglob(\"train.csv\"))\n",
        "    if candidates_csv:\n",
        "        print(f\"ðŸ”§ Moving CSV from {candidates_csv[0]} to {csv_path}\")\n",
        "        shutil.copy2(candidates_csv[0], csv_path)\n",
        "\n",
        "# Final checks\n",
        "problems = []\n",
        "if not train_dir.exists() or len(jpgs) == 0:\n",
        "    problems.append(\"train images missing\")\n",
        "\n",
        "if not csv_path.exists():\n",
        "    problems.append(\"train.csv missing\")\n",
        "\n",
        "if problems:\n",
        "    raise RuntimeError(\"âŒ Setup incomplete: \" + \", \".join(problems) +\n",
        "                       \"\\nPlease re-upload a valid archive or use Kaggle API.\")\n",
        "\n",
        "print(\"\\nâœ… Ready: dataset is prepared at /content/global-wheat-detection\")\n",
        "print(\"   - CSV:\", csv_path.exists(), \"| path:\", csv_path)\n",
        "print(\"   - Train images:\", len(jpgs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC-UCDdxofwS",
        "outputId": "a5ca4d6e-8f7d-49f4-d579-d5e5328abf35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Looking for dataset archives in /content ...\n",
            "Found: ['global-wheat-detection.zip', 'train.zip']\n",
            "ðŸ”Ž Testing global-wheat-detection.zip ...\n",
            "âœ… Zip integrity OK: global-wheat-detection.zip\n",
            "\n",
            "ðŸ“¦ Extracting global-wheat-detection.zip ... this may take a minute ...\n",
            "\n",
            "ðŸ–¼ï¸ Images found in /content/global-wheat-detection/train: 3422\n",
            "\n",
            "âœ… Ready: dataset is prepared at /content/global-wheat-detection\n",
            "   - CSV: True | path: /content/global-wheat-detection/train.csv\n",
            "   - Train images: 3422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3 â€” Parse bbox + 80/20 split (by image_id)\n",
        "\n",
        "import ast, random, glob\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "DATA_DIR = Path(\"/content/global-wheat-detection\")\n",
        "CSV_PATH = DATA_DIR / \"train.csv\"\n",
        "IMG_DIR  = DATA_DIR / \"train\"\n",
        "\n",
        "# 1) Load CSV\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "assert {'image_id','bbox','width','height'}.issubset(df.columns), \"CSV missing required columns.\"\n",
        "\n",
        "# 2) Keep only images that actually exist in IMG_DIR\n",
        "existing = {Path(p).stem for p in glob.glob(str(IMG_DIR / \"*.jpg\"))}\n",
        "df = df[df['image_id'].isin(existing)].copy()\n",
        "assert len(df) > 0, \"No matching rows after filtering to existing images.\"\n",
        "\n",
        "# 3) Parse bbox column \"[x, y, w, h]\" to separate numeric columns\n",
        "def parse_box(s):\n",
        "    x, y, w, h = ast.literal_eval(s)  # safer than eval\n",
        "    return x, y, w, h\n",
        "\n",
        "xywh = df['bbox'].apply(parse_box)\n",
        "df[['x','y','w','h']] = pd.DataFrame(xywh.tolist(), index=df.index)\n",
        "\n",
        "# 4) Build unique image list & 80/20 split\n",
        "image_ids = df['image_id'].drop_duplicates().tolist()\n",
        "random.shuffle(image_ids)\n",
        "split_idx = int(0.8 * len(image_ids))\n",
        "train_ids = image_ids[:split_idx]\n",
        "test_ids  = image_ids[split_idx:]\n",
        "\n",
        "# 5) Save splits for later steps\n",
        "SPLIT_DIR = Path(\"/content/wheat_split\")\n",
        "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(Path(SPLIT_DIR / \"train_ids.txt\")).write_text(\"\\n\".join(train_ids))\n",
        "(Path(SPLIT_DIR / \"test_ids.txt\")).write_text(\"\\n\".join(test_ids))\n",
        "\n",
        "# 6) Quick summary\n",
        "n_imgs = len(image_ids)\n",
        "n_tr, n_te = len(train_ids), len(test_ids)\n",
        "n_boxes_tr = df[df['image_id'].isin(train_ids)].shape[0]\n",
        "n_boxes_te = df[df['image_id'].isin(test_ids)].shape[0]\n",
        "\n",
        "print(\"âœ… Split complete\")\n",
        "print(f\"Total unique images: {n_imgs}\")\n",
        "print(f\"Train images: {n_tr}  | Annotations: {n_boxes_tr}\")\n",
        "print(f\"Test  images: {n_te}  | Annotations: {n_boxes_te}\")\n",
        "print(\"\\nSaved:\")\n",
        "print(f\"- /content/wheat_split/train_ids.txt\")\n",
        "print(f\"- /content/wheat_split/test_ids.txt\")\n",
        "\n",
        "# (We keep the parsed df in memory for exploration if you want:)\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "0TMu80XPoqLr",
        "outputId": "c251c589-e320-492c-bb2c-7151d6fcc269"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Split complete\n",
            "Total unique images: 3373\n",
            "Train images: 2698  | Annotations: 118152\n",
            "Test  images: 675  | Annotations: 29641\n",
            "\n",
            "Saved:\n",
            "- /content/wheat_split/train_ids.txt\n",
            "- /content/wheat_split/test_ids.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    image_id  width  height                         bbox   source      x  \\\n",
              "0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1  834.0   \n",
              "1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1  226.0   \n",
              "2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1  377.0   \n",
              "3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1  834.0   \n",
              "4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1   26.0   \n",
              "\n",
              "       y      w      h  \n",
              "0  222.0   56.0   36.0  \n",
              "1  548.0  130.0   58.0  \n",
              "2  504.0   74.0  160.0  \n",
              "3   95.0  109.0  107.0  \n",
              "4  144.0  124.0  117.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3bb7a20a-80fe-4308-9be2-59c70b0b568a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>bbox</th>\n",
              "      <th>source</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>w</th>\n",
              "      <th>h</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>1024</td>\n",
              "      <td>1024</td>\n",
              "      <td>[834.0, 222.0, 56.0, 36.0]</td>\n",
              "      <td>usask_1</td>\n",
              "      <td>834.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>36.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>1024</td>\n",
              "      <td>1024</td>\n",
              "      <td>[226.0, 548.0, 130.0, 58.0]</td>\n",
              "      <td>usask_1</td>\n",
              "      <td>226.0</td>\n",
              "      <td>548.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>1024</td>\n",
              "      <td>1024</td>\n",
              "      <td>[377.0, 504.0, 74.0, 160.0]</td>\n",
              "      <td>usask_1</td>\n",
              "      <td>377.0</td>\n",
              "      <td>504.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>160.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>1024</td>\n",
              "      <td>1024</td>\n",
              "      <td>[834.0, 95.0, 109.0, 107.0]</td>\n",
              "      <td>usask_1</td>\n",
              "      <td>834.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>107.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b6ab77fd7</td>\n",
              "      <td>1024</td>\n",
              "      <td>1024</td>\n",
              "      <td>[26.0, 144.0, 124.0, 117.0]</td>\n",
              "      <td>usask_1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>144.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>117.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3bb7a20a-80fe-4308-9be2-59c70b0b568a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3bb7a20a-80fe-4308-9be2-59c70b0b568a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3bb7a20a-80fe-4308-9be2-59c70b0b568a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0321be05-f248-41ed-a52a-62b3b51e807f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0321be05-f248-41ed-a52a-62b3b51e807f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0321be05-f248-41ed-a52a-62b3b51e807f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tYOLOv8 Training & Inference\n",
        "\n",
        "o\tUse the ultralytics library.\n",
        "\n",
        "o\tTrain YOLOv8 on the wheat dataset for at least 20 epochs.\n",
        "\n",
        "o\tEvaluate using mAP (mean Average Precision) and inference speed (FPS).\n"
      ],
      "metadata": {
        "id": "RH7QaKxVpOGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ultralytics YOLOv8 library\n",
        "!pip install ultralytics==8.3.0\n",
        "\n",
        "# Verify install\n",
        "import ultralytics\n",
        "print(\"Ultralytics version:\", ultralytics.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sptyL9sAphCh",
        "outputId": "06b00910-64cb-4448-cfb1-5cb73db7ad17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics==8.3.0 in /usr/local/lib/python3.12/dist-packages (8.3.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics==8.3.0) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->ultralytics==8.3.0) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics==8.3.0) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics==8.3.0) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics==8.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.0) (3.0.2)\n",
            "Ultralytics version: 8.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO label conversion (class = 0)\n",
        "\n",
        "YOLO wants normalized (cx, cy, w, h) in txt per image."
      ],
      "metadata": {
        "id": "T1dXfEsCxP0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2, os, tqdm\n",
        "\n",
        "IM_DIR = \"/content/global-wheat-detection/train\"  # folder with images (jpg)\n",
        "YOLO_IMG_DIR = f\"{DATA}/images\"\n",
        "YOLO_LBL_DIR = f\"{DATA}/labels\"\n",
        "for split in [\"train\",\"val\"]:\n",
        "    os.makedirs(f\"{YOLO_IMG_DIR}/{split}\", exist_ok=True)\n",
        "    os.makedirs(f\"{YOLO_LBL_DIR}/{split}\", exist_ok=True)\n",
        "\n",
        "val_ids = set(test_ids)  # use test as validation if you don't have a separate val set\n",
        "\n",
        "grouped = df.groupby(\"image_id\")\n",
        "for image_id, g in tqdm.tqdm(grouped):\n",
        "    img_path = f\"{IM_DIR}/{image_id}.jpg\"\n",
        "    if not os.path.exists(img_path):\n",
        "        continue\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "    # YOLO lines\n",
        "    lines = []\n",
        "    for _, r in g.iterrows():\n",
        "        x,y,bw,bh = r['x'], r['y'], r['w'], r['h']\n",
        "        cx = (x + bw/2) / w\n",
        "        cy = (y + bh/2) / h\n",
        "        nw = bw / w\n",
        "        nh = bh / h\n",
        "        lines.append(f\"0 {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\")\n",
        "\n",
        "    split = \"val\" if image_id in val_ids else \"train\"\n",
        "    cv2.imwrite(f\"{YOLO_IMG_DIR}/{split}/{image_id}.jpg\", img)\n",
        "    with open(f\"{YOLO_LBL_DIR}/{split}/{image_id}.txt\",\"w\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n"
      ],
      "metadata": {
        "id": "mogz22FLxQne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO data.yaml"
      ],
      "metadata": {
        "id": "I4CB3tfyxVHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_text = f\"\"\"\n",
        "path: {DATA}\n",
        "train: images/train\n",
        "val: images/val\n",
        "names:\n",
        "  0: wheat\n",
        "\"\"\"\n",
        "open(f\"{DATA}/wheat.yaml\",\"w\").write(yaml_text)\n",
        "print(open(f\"{DATA}/wheat.yaml\").read())\n"
      ],
      "metadata": {
        "id": "AlzsM-7ixVk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) YOLOv8 training & inference (Ultralytics)"
      ],
      "metadata": {
        "id": "4sen1BIHxZES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install ultralytics>=8.2.0 torchmetrics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "yolo_model = YOLO('yolov8s.pt')  # small, faster\n",
        "results = yolo_model.train(\n",
        "    data=f\"{DATA}/wheat.yaml\",\n",
        "    epochs=25,                 # â‰¥20 as required\n",
        "    imgsz=1024,                # Global Wheat is high-res; 1024 is a good compromise\n",
        "    batch=8,\n",
        "    device=0,\n",
        "    patience=5\n",
        ")\n"
      ],
      "metadata": {
        "id": "xbT9WxpExaVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO evaluation & speed"
      ],
      "metadata": {
        "id": "4iN0RMVjxcx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mAP reported by Ultralytics val automatically (check results.csv or print below)\n",
        "metrics = yolo_model.val()\n",
        "print(metrics)  # includes mAP50 and mAP50-95\n",
        "\n",
        "# Inference speed on ~100 images\n",
        "import glob, time\n",
        "test_imgs = sorted(glob.glob(f\"{YOLO_IMG_DIR}/val/*.jpg\"))[:100]\n",
        "t0 = time.time()\n",
        "_ = yolo_model(test_imgs, stream=False, verbose=False)  # warmup included\n",
        "t1 = time.time()\n",
        "ms_per_img = (t1 - t0) * 1000.0 / len(test_imgs)\n",
        "print(f\"YOLOv8s: {ms_per_img:.2f} ms/image  (~{1000/ms_per_img:.2f} FPS)\")\n"
      ],
      "metadata": {
        "id": "JLboFR2Pxd5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save qualitative outputs"
      ],
      "metadata": {
        "id": "D74UhBYHxhyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"/content/yolo_vis\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "for p in test_imgs[:8]:\n",
        "    res = yolo_model.predict(p, save=True, project=out_dir, name=\"preds\", exist_ok=True, verbose=False)\n",
        "print(\"Saved YOLO imgs under:\", out_dir)\n"
      ],
      "metadata": {
        "id": "YQq8dsQUxiVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Faster R-CNN training & inference (torchvision)\n",
        "\n",
        "Dataset for Faster R-CNN"
      ],
      "metadata": {
        "id": "raJcnik7xkz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "class WheatDetDataset(Dataset):\n",
        "    def __init__(self, df, ids, img_dir, transforms=None):\n",
        "        self.df = df\n",
        "        self.ids = ids\n",
        "        self.img_dir = img_dir\n",
        "        self.transforms = transforms\n",
        "        self.gdf = df.groupby(\"image_id\")\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.ids[idx]\n",
        "        img = Image.open(f\"{self.img_dir}/{image_id}.jpg\").convert(\"RGB\")\n",
        "        g = self.gdf.get_group(image_id)\n",
        "        boxes = []\n",
        "        for _, r in g.iterrows():\n",
        "            x,y,w,h = r['x'], r['y'], r['w'], r['h']\n",
        "            boxes.append([x, y, x+w, y+h])\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.ones((len(boxes),), dtype=torch.int64),  # class 1\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "        }\n",
        "        if self.transforms: img = self.transforms(img)\n",
        "        return img, target\n",
        "\n",
        "import torchvision.transforms as T\n",
        "train_ds = WheatDetDataset(df, [i for i in train_ids if (f\"{IM_DIR}/{i}.jpg\")], IM_DIR, T.ToTensor())\n",
        "val_ds   = WheatDetDataset(df, [i for i in test_ids  if (f\"{IM_DIR}/{i}.jpg\")], IM_DIR, T.ToTensor())\n",
        "\n",
        "def collate_fn(batch): return tuple(zip(*batch))\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=2)\n"
      ],
      "metadata": {
        "id": "djQOuX_ixl-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model, optimizer, training loop (~10â€“15 epochs)"
      ],
      "metadata": {
        "id": "p0AiLFuQxrLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "# set num_classes = 2 (background + wheat)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, 2)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    for imgs, targets in loader:\n",
        "        imgs = [im.to(device) for im in imgs]\n",
        "        targs = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
        "        loss_dict = model(imgs, targs)\n",
        "        loss = sum(loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item()\n",
        "    return loss_sum / len(loader)\n",
        "\n",
        "EPOCHS = 12\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr_loss = train_one_epoch(model, train_loader)\n",
        "    lr_sched.step()\n",
        "    print(f\"Epoch {ep}: loss {tr_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "loSH5WYrxsc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mAP with TorchMetrics"
      ],
      "metadata": {
        "id": "dEMDiQJixvUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torchmetrics>=1.3.0\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "metric = MeanAveragePrecision(iou_type=\"bbox\")  # computes mAP50 and mAP50-95\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, targets in val_loader:\n",
        "        imgs = [im.to(device) for im in imgs]\n",
        "        preds = model(imgs)\n",
        "        # convert to expected dicts on CPU\n",
        "        preds_cpu = []\n",
        "        for p in preds:\n",
        "            preds_cpu.append({\n",
        "                \"boxes\": p[\"boxes\"].detach().cpu(),\n",
        "                \"scores\": p[\"scores\"].detach().cpu(),\n",
        "                \"labels\": p[\"labels\"].detach().cpu(),\n",
        "            })\n",
        "        targs_cpu = [{k: v.detach().cpu() for k,v in t.items()} for t in targets]\n",
        "        metric.update(preds_cpu, targs_cpu)\n",
        "\n",
        "metrics_frcnn = metric.compute()\n",
        "print(metrics_frcnn)\n",
        "# Access like: metrics_frcnn['map_50'], metrics_frcnn['map']\n"
      ],
      "metadata": {
        "id": "f7Qr-9QXxxI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference speed"
      ],
      "metadata": {
        "id": "SMGJjEnix1jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, itertools\n",
        "\n",
        "def iter_images(ids, limit=100):\n",
        "    for i in ids[:limit]:\n",
        "        yield Image.open(f\"{IM_DIR}/{i}.jpg\").convert(\"RGB\")\n",
        "\n",
        "imgs = [T.ToTensor()(im).to(device) for im in iter_images(test_ids, 100)]\n",
        "torch.cuda.synchronize()\n",
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    # single-image predictions for fair ms/image\n",
        "    for im in imgs:\n",
        "        _ = model([im])\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "ms_per_img = (t1 - t0) * 1000.0 / len(imgs)\n",
        "print(f\"Faster R-CNN: {ms_per_img:.2f} ms/image (~{1000/ms_per_img:.2f} FPS)\")\n"
      ],
      "metadata": {
        "id": "rUlCrEUox2EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative images"
      ],
      "metadata": {
        "id": "FRC2KCZMx6Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, cv2\n",
        "vis_dir = \"/content/frcnn_vis\"; os.makedirs(vis_dir, exist_ok=True)\n",
        "model.eval()\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image_id in test_ids[:8]:\n",
        "        img = cv2.imread(f\"{IM_DIR}/{image_id}.jpg\")\n",
        "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        im_t = T.ToTensor()(Image.fromarray(rgb)).to(device)\n",
        "        pred = model([im_t])[0]\n",
        "        boxes = pred['boxes'].detach().cpu().numpy()\n",
        "        scores= pred['scores'].detach().cpu().numpy()\n",
        "        for (x1,y1,x2,y2), s in zip(boxes, scores):\n",
        "            if s<0.4: continue\n",
        "            cv2.rectangle(img,(int(x1),int(y1)),(int(x2),int(y2)),(0,255,0),2)\n",
        "            cv2.putText(img, f\"{s:.2f}\", (int(x1), max(0,int(y1)-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
        "        cv2.imwrite(f\"{vis_dir}/{image_id}.jpg\", img)\n",
        "print(\"Saved FRCNN imgs under:\", vis_dir)\n"
      ],
      "metadata": {
        "id": "hc_LBVNix7gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Results table (copy-paste and fill)\n",
        "\n",
        "| Model           | Epochs | mAP\\@0.5 | mAP\\@0.5:0.95 | ms/image (â†“) | Notes (good/missed) |\n",
        "| --------------- | -----: | -------: | ------------: | -----------: | ------------------- |\n",
        "| YOLOv8s         |     25 |          |               |              |                     |\n",
        "| Faster R-CNN-50 |     12 |          |               |              |                     |\n"
      ],
      "metadata": {
        "id": "NvDdZxK6x_gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation tips\n",
        "\n",
        "Expect YOLOv8 to be faster and often competitive or better at mAP50.\n",
        "\n",
        "Faster R-CNN may do better on mAP50â€“95 if trained long enough and tuned (anchors, LR, aug).\n",
        "\n",
        "Wheat heads are small/high-density; imgsz and augmentations impact results a lot."
      ],
      "metadata": {
        "id": "gZW1aERPyCHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tReport Preparation\n",
        "\n",
        "o\tSummarize findings in a short report.\n",
        "\n",
        "o\tInclude metrics, sample output images, and a discussion on which model worked          better and why.  \n"
      ],
      "metadata": {
        "id": "poh3G5vl0rQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Objective & Dataset (â‰ˆÂ¼ page)\n",
        "\n",
        "Goal. Compare YOLOv8s (one-stage) vs Faster R-CNN ResNet50-FPN (two-stage) on Global Wheat Detection in accuracy, speed, and robustness.\n",
        "Dataset. Global Wheat Detection (Kaggle): 1 class (wheat head), ~3,300 images with bounding boxes [x, y, w, h].\n",
        "Split. Train [80%], Val/Test [20%] (image-wise).\n",
        "Preprocessing/labels. YOLO: normalized (cx, cy, w, h); Faster R-CNN: [x1, y1, x2, y2]. Image size used: [imgsz]."
      ],
      "metadata": {
        "id": "xZFvj_L800KB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Methods (â‰ˆÂ½ page)\n",
        "\n",
        "YOLOv8s (Ultralytics).\n",
        "\n",
        "Hyperparams: epochs [â‰¥20, used: XX], imgsz [1024], batch [8], optimizer/aug: defaults.\n",
        "\n",
        "Training command/log reference: [path/to/runs/train/â€¦].\n",
        "\n",
        "Faster R-CNN (torchvision).\n",
        "\n",
        "Hyperparams: epochs [10â€“15, used: XX], batch [4], LR schedule [0.005 â†’ StepLR(5, 0.1)].\n",
        "\n",
        "Implementation: torchvision.models.detection.fasterrcnn_resnet50_fpn, predictor set to 2 classes (bg + wheat).\n",
        "\n",
        "Evaluation protocol (same split & hardware).\n",
        "\n",
        "Metrics: mAP@0.5, mAP@0.5:0.95.\n",
        "\n",
        "Speed: ms/image (report single-image inference; warm-up + torch.cuda.synchronize())."
      ],
      "metadata": {
        "id": "kXKvmUga022e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Results (Quantitative) (â‰ˆÂ½ page)\n",
        "\n",
        "| Model           | Epochs | mAP\\@0.5 | mAP\\@0.5:0.95 | ms/image (â†“) | Notes                          |\n",
        "| --------------- | -----: | -------: | ------------: | -----------: | ------------------------------ |\n",
        "| YOLOv8s         |   \\[ ] |     \\[ ] |          \\[ ] |         \\[ ] | \\[e.g., strong on clear heads] |\n",
        "| Faster R-CNN-50 |   \\[ ] |     \\[ ] |          \\[ ] |         \\[ ] | \\[e.g., tighter boxes?]        |\n"
      ],
      "metadata": {
        "id": "yq7OtaCD0-eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Results (Qualitative) (â‰ˆÂ½ page)\n",
        "\n",
        "Good detections (insert 2â€“3 images per model):\n",
        "\n",
        "YOLOv8: [image + one-line caption].\n",
        "\n",
        "Faster R-CNN: [image + one-line caption].\n",
        "\n",
        "Missed/Failure cases (tiny heads, overlaps, occlusion, background FPs):\n",
        "\n",
        "YOLOv8: [image + one-line caption].\n",
        "\n",
        "Faster R-CNN: [image + one-line caption]."
      ],
      "metadata": {
        "id": "5EusNPlq1AtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Discussion & Conclusion (â‰ˆÂ½ page)\n",
        "\n",
        "Which performed better and why.\n",
        "\n",
        "Accuracy: At mAP@0.5, [winner] by [Î”]. At mAP@0.5:0.95, [winner] by [Î”].\n",
        "\n",
        "Speed: [model] faster by [Ã—] ( [ms/img] vs [ms/img] ).\n",
        "\n",
        "Interpretation: One-stage YOLOv8 is typically faster and competitive at mAP@0.5; two-stage Faster R-CNN can improve localization precision (benefits 0.5:0.95) given enough training/tuning.\n",
        "\n",
        "Robustness: [who handled tiny/overlapping heads better? Any systematic FPs?]\n",
        "\n",
        "What to try next.\n",
        "\n",
        "Higher imgsz (1280) for small objects; longer training ( +10â€“20 epochs ).\n",
        "\n",
        "Stronger backbones (YOLOv8m/l, ResNet101-FPN).\n",
        "\n",
        "Augmentations for density: Mosaic, RandomCrop, Copy-Paste.\n",
        "\n",
        "Post-processing sweeps: conf/NMS thresholds."
      ],
      "metadata": {
        "id": "_8yGBmAY1FCU"
      }
    }
  ]
}