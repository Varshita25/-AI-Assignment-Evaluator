{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":19989,"databundleVersionId":1160143,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Global Wheat Detection: YOLOv8 vs Faster R-CNN  \n# ============================================================\n\n# --- Clean & pin compatible packages ---\n!pip -q uninstall -y ray ray-core ray[default] || true      # remove Ray to avoid Ultralytics callback crash\n!pip -q uninstall -y albumentations albucore -y             # remove conflicting installs of albumentations/albucore\n!pip -q install --upgrade \"ultralytics==8.3.29\" \"pycocotools\" \"torchmetrics\" \\\n                             \"opencv-python-headless==4.10.0.84\" \\\n                             \"albumentations==1.3.1\"         # albumentations pre-albucore split\n\n# --- Imports & set env flags ---\nimport os, gc, re, ast, time, random, shutil             # stdlib helpers\nfrom pathlib import Path                                  # path ops\nimport numpy as np                                        # arrays\nimport pandas as pd                                       # tables\nfrom PIL import Image, ImageDraw                          # image IO/annotation\nfrom tqdm.auto import tqdm                                 # progress bars\n\nos.environ[\"ULTRALYTICS_RAY\"] = \"0\"                       # hard-disable Ray callbacks in Ultralytics\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision.transforms import functional as TF\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom ultralytics import YOLO\n\n# --------------------------\n# Reproducibility & device\n# --------------------------\nSEED = 42                                                 # random seed for reproducibility\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# --------------------------\n# Config section given below\n# --------------------------\nEPOCHS_YOLO   = 20      \nEPOCHS_FRCNN  = 6      # making it 6 to complete quick\nBATCH_FRCNN   = 2      # small batch -> less VRAM\nIMGSZ         = 640    # smaller images than 1024 to save RAM/VRAM\nNUM_WORKERS   = 1      # 1 worker keeps RAM usage modest\nVAL_FRACTION  = 0.2    # 80/20 split\nN_QUAL        = 8      # number of qualitative samples to save\nSPEED_SAMPLES = 60     # images for speed tests\nCLASS_NAME    = \"wheat_head\"  # single-class detection\n\n# --- Working directories ---\nWORKDIR        = Path(\"/kaggle/working\")\nYOLO_DIR       = WORKDIR / \"wheat_yolo\"\nYOLO_IMG_DIR_TR= YOLO_DIR/\"images/train\"\nYOLO_IMG_DIR_VA= YOLO_DIR/\"images/val\"\nYOLO_LAB_DIR_TR= YOLO_DIR/\"labels/train\"\nYOLO_LAB_DIR_VA= YOLO_DIR/\"labels/val\"\nOUT_DIR        = WORKDIR / \"outputs\"\nfor p in [YOLO_IMG_DIR_TR, YOLO_IMG_DIR_VA, YOLO_LAB_DIR_TR, YOLO_LAB_DIR_VA, OUT_DIR]:\n    p.mkdir(parents=True, exist_ok=True)                  # create folders\n\n# --------------------------\n# Locate dataset in Kaggle\n# --------------------------\ndef find_wheat_input():\n    \"\"\"Return path to Global Wheat Detection dataset inside /kaggle/input.\"\"\"\n    cand = Path(\"/kaggle/input/global-wheat-detection\")\n    if cand.exists(): return cand\n    for d in Path(\"/kaggle/input\").glob(\"*\"):\n        if (d/\"train.csv\").exists() and (d/\"train\").exists():\n            return d\n    raise FileNotFoundError(\"Global Wheat Detection dataset not found under /kaggle/input\")\n\nDATA_ROOT    = find_wheat_input()\nprint(\"Dataset root:\", DATA_ROOT)\n\nTRAIN_IMG_DIR= DATA_ROOT / \"train\"   # training images\nTRAIN_CSV    = DATA_ROOT / \"train.csv\"# annotations (bbox per row)\nTEST_IMG_DIR = DATA_ROOT / \"test\"    # optional visuals\n\n# --- Load annotations ---\ndf = pd.read_csv(TRAIN_CSV)                      # read bbox file\nprint(\"Train CSV shape:\", df.shape)\ndisplay(df.head())\n\n# --------------------------\n# Parse bbox strings -> [x,y,w,h]\n# --------------------------\ndef parse_bbox_str(s):\n    \"\"\"Parse '[x, y, w, h]' strings robustly into list of floats.\"\"\"\n    s = str(s).strip()\n    if s.startswith(\"[\") and s.endswith(\"]\"):\n        return list(map(float, ast.literal_eval(s)))\n    nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)   # fallback: regex numbers\n    return list(map(float, nums[:4]))\n\ndf[\"bbox_parsed\"] = df[\"bbox\"].apply(parse_bbox_str)   # convert bbox column\nimg_meta = df.groupby(\"image_id\")[[\"width\",\"height\"]].first().to_dict(\"index\")  # size per image\n\n# --- Train/Val split by image_id to avoid leak ---\nimage_ids = list(img_meta.keys()); random.shuffle(image_ids)\nn_val     = int(len(image_ids)*VAL_FRACTION)\nval_ids   = set(image_ids[:n_val])\ntrain_ids = set(image_ids[n_val:])\nprint(\"Train images:\", len(train_ids), \"Val images:\", len(val_ids))\n\n# ======================================================\n# A) Prepare YOLO format (txt per image: cls xc yc w h)\n# ======================================================\ndef coco_to_yolo(xywh, img_w, img_h):\n    \"\"\"COCO [x,y,w,h] -> YOLO normalized [xc,yc,w,h].\"\"\"\n    x, y, w, h = xywh\n    xc = x + w/2.0; yc = y + h/2.0\n    return [xc/img_w, yc/img_h, w/img_w, h/img_h]\n\ndef write_yolo_label(lbl_path, rows, img_w, img_h):\n    \"\"\"Write a YOLO .txt with one line per bbox: '0 xc yc w h'.\"\"\"\n    with open(lbl_path, \"w\") as f:\n        for xywh in rows:\n            xc,yc,ww,hh = coco_to_yolo(xywh, img_w, img_h)\n            f.write(f\"0 {xc:.6f} {yc:.6f} {ww:.6f} {hh:.6f}\\n\")\n\ndef prepare_yolo_split(split_ids, img_target_dir, lab_target_dir):\n    \"\"\"Symlink images and emit YOLO label files for a set of image_ids.\"\"\"\n    count = 0\n    for img_id in tqdm(split_ids, desc=f\"YOLO prep -> {img_target_dir.parent.name}\"):\n        src = TRAIN_IMG_DIR/f\"{img_id}.jpg\"\n        if not src.exists(): \n            continue\n        dst_img = img_target_dir/f\"{img_id}.jpg\"\n        if not dst_img.exists():\n            os.symlink(src, dst_img)                       # link image (fast, low disk)\n        rows = df[df.image_id==img_id][\"bbox_parsed\"].tolist()\n        imw, imh = img_meta[img_id][\"width\"], img_meta[img_id][\"height\"]\n        write_yolo_label(lab_target_dir/f\"{img_id}.txt\", rows, imw, imh)\n        count += 1\n    return count\n\n# --- Build YOLO train/val folders & labels ---\nn_tr = prepare_yolo_split(train_ids, YOLO_IMG_DIR_TR, YOLO_LAB_DIR_TR)\nn_va = prepare_yolo_split(val_ids,   YOLO_IMG_DIR_VA, YOLO_LAB_DIR_VA)\nprint(f\"YOLO files -> train:{n_tr}, val:{n_va}\")\n\n# --- YOLO data.yaml describing paths/class names ---\n(DATA_YAML := YOLO_DIR/\"wheat.yaml\").write_text(f\"\"\"\\\npath: {YOLO_DIR.as_posix()}\ntrain: images/train\nval: images/val\nnc: 1\nnames: ['{CLASS_NAME}']\n\"\"\")\nprint(\"Wrote\", DATA_YAML)\n\n# ======================================================\n# B) YOLOv8 training, validation, speed, qualitative\n# ======================================================\nyolo_model = YOLO(\"yolov8n.pt\")  # small model for speed; try 'yolov8s.pt' later\n\n# --- Train YOLO (cache to disk to reduce RAM) ---\nyolo_results = yolo_model.train(\n    data=str(DATA_YAML),\n    epochs=EPOCHS_YOLO,\n    imgsz=IMGSZ,\n    cache=\"disk\",\n    batch=8,\n    workers=2,\n    device=0 if device.type==\"cuda\" else \"cpu\",\n    project=str(WORKDIR/\"ultra_runs\"),\n    name=\"yolo_wheat\",\n    patience=3,                           # early stop if no improvement\n    verbose=True\n)\n\n# --- Best checkpoint path (for logs/info only) ---\nbest_dir = Path(yolo_results.save_dir)\nbest_pt  = best_dir/\"weights/best.pt\"\nprint(\"YOLO best:\", best_pt if best_pt.exists() else \"unknown\")\n\n# --- Validate YOLO & extract mAP metrics robustly across versions ---\nyolo_map50 = np.nan; yolo_map5095 = np.nan\ntry:\n    yres = yolo_model.val(workers=2, imgsz=IMGSZ, plots=False, verbose=False)\n    if hasattr(yres, \"metrics\") and hasattr(yres.metrics, \"box\"):\n        yolo_map50   = float(getattr(yres.metrics.box, \"map50\", np.nan))\n        yolo_map5095 = float(getattr(yres.metrics.box, \"map\",   np.nan))\n    elif hasattr(yres, \"results_dict\"):\n        rd = yres.results_dict\n        yolo_map50   = float(rd.get(\"metrics/mAP50(B)\",     np.nan))\n        yolo_map5095 = float(rd.get(\"metrics/mAP50-95(B)\",  np.nan))\nexcept Exception as e:\n    print(\"YOLO val metric parse warning:\", e)\n\nprint(\"YOLO mAP50:\", yolo_map50, \"mAP50-95:\", yolo_map5095)\n\n# --- Measure YOLO inference speed on a subset ---\ndef measure_yolo_speed(model, img_dir, num=60):\n    paths = list(Path(img_dir).glob(\"*.jpg\"))\n    if not paths: \n        return np.nan, np.nan\n    paths = random.sample(paths, min(num, len(paths)))\n    _ = model.predict(source=str(paths[0]), imgsz=IMGSZ, verbose=False)  # warmup\n    t0 = time.time()\n    _  = model.predict(source=[str(p) for p in paths], imgsz=IMGSZ, verbose=False)\n    dt = time.time()-t0\n    tpi = dt/len(paths)\n    return 1.0/tpi, tpi\n\nyolo_fps, yolo_tpi = measure_yolo_speed(yolo_model, YOLO_IMG_DIR_VA, num=SPEED_SAMPLES)\nprint(f\"YOLO speed -> {yolo_fps:.2f} FPS ({yolo_tpi*1000:.1f} ms/img)\")\n\n# --- Save YOLO qualitative predictions (annotated images) ---\n(yolo_pred_dir := OUT_DIR/\"qual_yolo\").mkdir(exist_ok=True, parents=True)\n_ = yolo_model.predict(source=str(YOLO_IMG_DIR_VA), save=True, project=str(yolo_pred_dir),\n                       name=\"preds\", imgsz=IMGSZ, max_det=300, conf=0.25, verbose=False)\nprint(\"YOLO qualitative saved to:\", yolo_pred_dir)\n\n# --- Free GPU/CPU RAM before Faster R-CNN ---\ndel yolo_results; gc.collect()\ntry:\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\nexcept: pass\n\n# ======================================================\n# C) Faster R-CNN dataset, training, eval, speed, qualitative\n# ======================================================\nclass WheatDataset(Dataset):\n    \"\"\"Minimal detection dataset for torchvision's Faster R-CNN.\"\"\"\n    def __init__(self, ids, img_dir, df, transforms=None):\n        self.ids = list(ids)\n        self.img_dir = Path(img_dir)\n        self.df = df\n        self.transforms = transforms\n        self.group = df.groupby(\"image_id\")[\"bbox_parsed\"].apply(list).to_dict()\n\n    def __len__(self): \n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id  = self.ids[idx]\n        img_path= self.img_dir/f\"{img_id}.jpg\"\n        image   = np.array(Image.open(img_path).convert(\"RGB\"))\n        boxes   = np.array(self.group[img_id], dtype=np.float32)   # [x,y,w,h]\n\n        # convert to [x1,y1,x2,y2] as expected by torchvision\n        boxes_xyxy      = boxes.copy()\n        boxes_xyxy[:,2] = boxes[:,0] + boxes[:,2]\n        boxes_xyxy[:,3] = boxes[:,1] + boxes[:,3]\n        boxes_xyxy[:,0] = boxes[:,0]\n        boxes_xyxy[:,1] = boxes[:,1]\n        labels          = np.ones((len(boxes_xyxy),), dtype=np.int64)  # single class=1\n\n        target = {\n            \"boxes\":    torch.as_tensor(boxes_xyxy, dtype=torch.float32),\n            \"labels\":   torch.as_tensor(labels,      dtype=torch.int64),\n            \"image_id\": torch.tensor([idx])\n        }\n\n        if self.transforms:\n            transformed = self.transforms(image=image, bboxes=boxes_xyxy, class_labels=labels.tolist())\n            image = transformed[\"image\"]\n            bxs   = transformed[\"bboxes\"]; cl = transformed[\"class_labels\"]\n            if len(bxs) == 0:\n                target[\"boxes\"]  = torch.zeros((0,4), dtype=torch.float32)\n                target[\"labels\"] = torch.zeros((0,),   dtype=torch.int64)\n            else:\n                target[\"boxes\"]  = torch.tensor(bxs, dtype=torch.float32)\n                target[\"labels\"] = torch.tensor(cl,  dtype=torch.int64)\n        else:\n            image = TF.to_tensor(Image.fromarray(image))\n        return image, target\n\n# --- Albumentations transforms â€” ensure float tensors for torchvision ---\ntrain_tfms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.15),\n    A.LongestMaxSize(max_size=IMGSZ, p=1.0),\n    A.PadIfNeeded(min_height=IMGSZ, min_width=IMGSZ, border_mode=0, value=(114,114,114)),\n    A.ToFloat(max_value=255.0),     # convert to float [0,1]\n    ToTensorV2()\n], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]))\n\nval_tfms = A.Compose([\n    A.LongestMaxSize(max_size=IMGSZ, p=1.0),\n    A.PadIfNeeded(min_height=IMGSZ, min_width=IMGSZ, border_mode=0, value=(114,114,114)),\n    A.ToFloat(max_value=255.0),     # convert to float [0,1]\n    ToTensorV2()\n], bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]))\n\n# --- Build datasets/dataloaders (memory-conscious loader settings) ---\nds_train = WheatDataset(train_ids, TRAIN_IMG_DIR, df, transforms=train_tfms)\nds_val   = WheatDataset(val_ids,   TRAIN_IMG_DIR, df, transforms=val_tfms)\n\ndef collate_fn(batch): \n    return tuple(zip(*batch))        # torchvision detection expects lists, not tensors\n\ndl_train = DataLoader(ds_train, batch_size=BATCH_FRCNN, shuffle=True,\n                      num_workers=NUM_WORKERS, collate_fn=collate_fn,\n                      pin_memory=False, persistent_workers=False)\ndl_val   = DataLoader(ds_val,   batch_size=BATCH_FRCNN, shuffle=False,\n                      num_workers=NUM_WORKERS, collate_fn=collate_fn,\n                      pin_memory=False, persistent_workers=False)\n\n# --- Model: Faster R-CNN ResNet50 FPN, replace head for 2 classes (bg + wheat) ---\nfrcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\nin_feats = frcnn.roi_heads.box_predictor.cls_score.in_features\nfrcnn.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_feats, 2)\nfrcnn = frcnn.to(device)\n\n# --- Optimizer & LR schedule ---\nparams       = [p for p in frcnn.parameters() if p.requires_grad]\noptimizer    = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n\n# --- Train loop (ensure float tensors) ---\ndef train_frcnn(model, dl, epochs):\n    model.train()\n    for epoch in range(epochs):\n        losses_epoch = []\n        pbar = tqdm(dl, desc=f\"FRCNN Epoch {epoch+1}/{epochs}\")\n        for images, targets in pbar:\n            images  = [img.float().to(device) for img in images]  # make sure images are float32\n            targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)            # returns dict of losses\n            loss = sum(loss_dict.values())                # scalar total loss\n\n            optimizer.zero_grad(); loss.backward(); optimizer.step()\n            losses_epoch.append(loss.item())\n            pbar.set_postfix(loss=np.mean(losses_epoch))\n        lr_scheduler.step()\n        print(f\"Epoch {epoch+1} mean loss: {np.mean(losses_epoch):.4f}\")\n\ntrain_frcnn(frcnn, dl_train, EPOCHS_FRCNN)\n\n# --- Evaluate mAP (0.5 and 0.5:0.95) with torchmetrics ---\n@torch.no_grad()\ndef eval_map(model, dl):\n    model.eval()\n    metric = MeanAveragePrecision(iou_type=\"bbox\")\n    for images, targets in tqdm(dl, desc=\"Eval FRCNN\"):\n        images_gpu = [img.float().to(device) for img in images]  # float\n        preds = model(images_gpu)\n        preds_cpu, targets_cpu = [], []\n        for p, t in zip(preds, targets):\n            preds_cpu.append({\n                \"boxes\":  p[\"boxes\"].detach().cpu(),\n                \"scores\": p[\"scores\"].detach().cpu(),\n                \"labels\": p[\"labels\"].detach().cpu()\n            })\n            targets_cpu.append({\n                \"boxes\":  t[\"boxes\"].cpu(),\n                \"labels\": t[\"labels\"].cpu()\n            })\n        metric.update(preds_cpu, targets_cpu)\n    res = metric.compute()\n    return float(res[\"map_50\"]), float(res[\"map\"])\n\nfrcnn_map50, frcnn_map5095 = eval_map(frcnn, dl_val)\nprint(\"FRCNN mAP50:\", frcnn_map50, \"mAP50-95:\", frcnn_map5095)\n\n# --- Measure Faster R-CNN inference speed ---\n@torch.no_grad()\ndef frcnn_speed(model, ds, num=60):\n    model.eval()\n    idxs = random.sample(range(len(ds)), min(num, len(ds)))\n    img,_ = ds[idxs[0]]; _ = model([img.float().to(device)])  # warmup\n    t0 = time.time()\n    for i in idxs:\n        img,_ = ds[i]; _ = model([img.float().to(device)])\n    dt  = time.time() - t0\n    tpi = dt/len(idxs)\n    return 1.0/tpi, tpi\n\nfrcnn_fps, frcnn_tpi = frcnn_speed(frcnn, ds_val, num=SPEED_SAMPLES)\nprint(f\"FRCNN speed -> {frcnn_fps:.2f} FPS ({frcnn_tpi*1000:.1f} ms/img)\")\n\n# --- Save qualitative predictions for Faster R-CNN ---\ndef draw_boxes(img_path, boxes, scores=None, thr=0.25):\n    \"\"\"Draw rectangles for predicted boxes (optionally filtered by score).\"\"\"\n    im = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(im)\n    for i, b in enumerate(boxes):\n        if scores is not None and scores[i] < thr: \n            continue\n        x1,y1,x2,y2 = map(float, b)\n        draw.rectangle([x1,y1,x2,y2], outline=(0,255,0), width=2)\n    return im\n\n(qual_dir_frcnn := OUT_DIR/\"qual_frcnn\").mkdir(parents=True, exist_ok=True)\n\n@torch.no_grad()\ndef save_qualitative(model, ids, n=8, outdir=qual_dir_frcnn):\n    \"\"\"Save n validation images with predicted boxes.\"\"\"\n    sel = random.sample(list(ids), min(n, len(ids)))\n    model.eval()\n    for img_id in sel:\n        p   = TRAIN_IMG_DIR/f\"{img_id}.jpg\"\n        img = TF.to_tensor(Image.open(p).convert(\"RGB\")).float().to(device)\n        pred= model([img])[0]\n        im  = draw_boxes(p, pred[\"boxes\"].detach().cpu().numpy(),\n                         pred[\"scores\"].detach().cpu().numpy(), thr=0.25)\n        im.save(outdir/f\"{img_id}_pred.jpg\")\n\nsave_qualitative(frcnn, val_ids, n=N_QUAL)\nprint(\"FRCNN qualitative saved to:\", qual_dir_frcnn)\n\n# ======================================================\n# D) Final comparison table \n# ======================================================\ncomp = pd.DataFrame([\n    {\"Model\":\"YOLOv8n\",\n     \"mAP@0.5\": round(yolo_map50, 4),\n     \"mAP@0.5:0.95\": round(yolo_map5095, 4),\n     \"Time/Image (s)\": round(yolo_tpi, 4),\n     \"FPS\": round(yolo_fps, 2)},\n    {\"Model\":\"Faster R-CNN (R50-FPN)\",\n     \"mAP@0.5\": round(frcnn_map50, 4),\n     \"mAP@0.5:0.95\": round(frcnn_map5095, 4),\n     \"Time/Image (s)\": round(frcnn_tpi, 4),\n     \"FPS\": round(frcnn_fps, 2)}\n])\nprint(\"\\n=== Results Comparison ===\")\nprint(comp.to_string(index=False))\nOUT_DIR.mkdir(exist_ok=True, parents=True)\ncomp.to_csv(OUT_DIR/\"comparison.csv\", index=False)\nprint(\"\\nSaved comparison CSV to:\", OUT_DIR/\"comparison.csv\")\nprint(\"\\nQualitative results:\\n - YOLOv8:\", OUT_DIR/'qual_yolo', \"\\n - FRCNN  :\", OUT_DIR/'qual_frcnn')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-20T13:34:35.657284Z","iopub.execute_input":"2025-09-20T13:34:35.657548Z","iopub.status.idle":"2025-09-20T17:09:53.499799Z","shell.execute_reply.started":"2025-09-20T13:34:35.657529Z","shell.execute_reply":"2025-09-20T17:09:53.498816Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping ray-core as it is not installed.\u001b[0m\u001b[33m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.8/883.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.7/125.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nDevice: cuda\nDataset root: /kaggle/input/global-wheat-detection\nTrain CSV shape: (147793, 5)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    image_id  width  height                         bbox   source\n0  b6ab77fd7   1024    1024   [834.0, 222.0, 56.0, 36.0]  usask_1\n1  b6ab77fd7   1024    1024  [226.0, 548.0, 130.0, 58.0]  usask_1\n2  b6ab77fd7   1024    1024  [377.0, 504.0, 74.0, 160.0]  usask_1\n3  b6ab77fd7   1024    1024  [834.0, 95.0, 109.0, 107.0]  usask_1\n4  b6ab77fd7   1024    1024  [26.0, 144.0, 124.0, 117.0]  usask_1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>width</th>\n      <th>height</th>\n      <th>bbox</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 222.0, 56.0, 36.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[226.0, 548.0, 130.0, 58.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[377.0, 504.0, 74.0, 160.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[834.0, 95.0, 109.0, 107.0]</td>\n      <td>usask_1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6ab77fd7</td>\n      <td>1024</td>\n      <td>1024</td>\n      <td>[26.0, 144.0, 124.0, 117.0]</td>\n      <td>usask_1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Train images: 2699 Val images: 674\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"YOLO prep -> images:   0%|          | 0/2699 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e3d0b78ee5439393e3287576febd61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"YOLO prep -> images:   0%|          | 0/674 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130c5738544d402eb61eee0f1e9f0a43"}},"metadata":{}},{"name":"stdout","text":"YOLO files -> train:2699, val:674\nWrote /kaggle/working/wheat_yolo/wheat.yaml\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.25M/6.25M [00:00<00:00, 82.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"New https://pypi.org/project/ultralytics/8.3.202 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\nUltralytics 8.3.29 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/working/wheat_yolo/wheat.yaml, epochs=20, time=None, patience=3, batch=8, imgsz=640, save=True, save_period=-1, cache=disk, device=0, workers=2, project=/kaggle/working/ultra_runs, name=yolo_wheat, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/kaggle/working/ultra_runs/yolo_wheat\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 16.9MB/s]\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758375451.700899      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758375451.808760      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /kaggle/working/ultra_runs/yolo_wheat', view at http://localhost:6006/\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 72.9MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/wheat_yolo/labels/train... 2699 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2699/2699 [00:08<00:00, 330.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/wheat_yolo/labels/train.cache\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (7.9GB Disk): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2699/2699 [00:17<00:00, 156.96it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/wheat_yolo/labels/val... 674 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:02<00:00, 289.50it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/wheat_yolo/labels/val.cache\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.0GB Disk): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:04<00:00, 144.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to /kaggle/working/ultra_runs/yolo_wheat/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1m/kaggle/working/ultra_runs/yolo_wheat\u001b[0m\nStarting training for 20 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       1/20      2.65G      1.911      1.463      1.388        136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:41<00:00,  8.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:07<00:00,  5.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721       0.82      0.777      0.836      0.397\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       2/20      3.24G      1.733     0.9965      1.264        133        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:37<00:00,  9.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  6.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.849      0.794      0.868      0.453\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       3/20      2.25G      1.717     0.9754      1.265        341        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:37<00:00,  9.13it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  6.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.814      0.779      0.841      0.432\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       4/20      2.17G       1.68     0.9221       1.24        156        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.16it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  6.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721       0.84      0.809      0.875      0.464\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       5/20      2.45G      1.662     0.9018      1.238        156        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.20it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.886      0.842      0.911       0.49\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       6/20      1.99G      1.647     0.8715       1.23        195        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.15it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.888      0.845      0.909      0.496\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       7/20      1.81G      1.644     0.8582      1.225        245        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.21it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.891      0.846      0.914      0.512\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       8/20      2.66G      1.623     0.8419      1.222        171        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.22it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.899      0.861      0.921      0.515\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"       9/20      2.56G      1.616      0.831      1.222        127        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.23it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  6.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721       0.89      0.845      0.912      0.501\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      10/20      2.37G      1.605     0.8155      1.208        125        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:36<00:00,  9.17it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.899      0.859      0.924      0.526\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      11/20      2.12G      1.606     0.8174      1.242        143        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:35<00:00,  9.60it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.893      0.849      0.918      0.521\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      12/20      1.91G      1.576      0.789      1.231        114        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.903      0.855      0.924      0.529\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      13/20       1.6G      1.573     0.7828      1.228        144        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.81it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.903      0.862      0.928      0.532\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      14/20      1.82G      1.556     0.7721      1.223         87        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.909      0.871      0.932      0.534\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      15/20      1.61G      1.555     0.7591      1.217         98        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.80it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:05<00:00,  7.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.909      0.872      0.933      0.542\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      16/20      1.72G      1.541     0.7527      1.213        197        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.86it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721       0.91      0.879      0.936      0.542\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      17/20      1.39G      1.534      0.738      1.207         88        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721       0.91      0.878      0.936      0.543\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      18/20      1.95G      1.526     0.7325      1.206         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.915      0.879      0.938      0.548\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      19/20      1.68G      1.515     0.7226        1.2        100        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.75it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.913      0.881      0.938      0.549\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      20/20      1.72G      1.509     0.7161      1.196        161        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338/338 [00:34<00:00,  9.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:06<00:00,  7.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.915      0.882      0.939      0.552\n\n20 epochs completed in 0.241 hours.\nOptimizer stripped from /kaggle/working/ultra_runs/yolo_wheat/weights/last.pt, 6.2MB\nOptimizer stripped from /kaggle/working/ultra_runs/yolo_wheat/weights/best.pt, 6.2MB\n\nValidating /kaggle/working/ultra_runs/yolo_wheat/weights/best.pt...\nUltralytics 8.3.29 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:16<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.915      0.883      0.939      0.552\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","output_type":"stream"},{"name":"stdout","text":"Speed: 0.2ms preprocess, 3.1ms inference, 0.0ms loss, 4.1ms postprocess per image\nResults saved to \u001b[1m/kaggle/working/ultra_runs/yolo_wheat\u001b[0m\nYOLO best: /kaggle/working/ultra_runs/yolo_wheat/weights/best.pt\nUltralytics 8.3.29 ðŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/wheat_yolo/labels/val.cache... 674 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:00<?, ?it/s]\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.0GB Disk): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:00<00:00, 27207.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:06<00:00, 13.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all        674      29721      0.915      0.882      0.939      0.552\nSpeed: 0.3ms preprocess, 3.9ms inference, 0.0ms loss, 1.2ms postprocess per image\nYOLO mAP50: 0.9391368556481577 mAP50-95: 0.552491950751263\nYOLO speed -> 29.78 FPS (33.6 ms/img)\nResults saved to \u001b[1m/kaggle/working/outputs/qual_yolo/preds\u001b[0m\nYOLO qualitative saved to: /kaggle/working/outputs/qual_yolo\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160M/160M [00:00<00:00, 187MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 1/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85d37405106422988526dd31c06aae2"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 mean loss: 0.8915\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 2/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86451dd267bb4ac191f2d9a4f40caf60"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 mean loss: 0.7874\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 3/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22cb60eaeefe4ae69685eaf27f50c9a4"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 mean loss: 0.7588\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 4/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa0b12af40c40f2a8bee1fd293579db"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 mean loss: 0.7389\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 5/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5887e8ef631242aa85a725731d7dc139"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 mean loss: 0.6831\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FRCNN Epoch 6/6:   0%|          | 0/1350 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a6018563748407c95e1c784b11db9c8"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 mean loss: 0.6738\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval FRCNN:   0%|          | 0/337 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df61528618714305870fcd2f2d166961"}},"metadata":{}},{"name":"stderr","text":"Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d9c36e116c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d9c36e116c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d9c36e116c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7d9c36e116c0>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n","output_type":"stream"},{"name":"stdout","text":"FRCNN mAP50: 0.922092854976654 mAP50-95: 0.5110437870025635\nFRCNN speed -> 8.82 FPS (113.4 ms/img)\nFRCNN qualitative saved to: /kaggle/working/outputs/qual_frcnn\n\n=== Results Comparison ===\n                 Model  mAP@0.5  mAP@0.5:0.95  Time/Image (s)   FPS\n               YOLOv8n   0.9391        0.5525          0.0336 29.78\nFaster R-CNN (R50-FPN)   0.9221        0.5110          0.1134  8.82\n\nSaved comparison CSV to: /kaggle/working/outputs/comparison.csv\n\nQualitative results:\n - YOLOv8: /kaggle/working/outputs/qual_yolo \n - FRCNN  : /kaggle/working/outputs/qual_frcnn\n","output_type":"stream"}],"execution_count":1}]}